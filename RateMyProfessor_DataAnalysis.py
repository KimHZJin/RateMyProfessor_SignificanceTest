# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1msn5YKrYkNTck0jcrQrB0k8DKgrICf2t

# Questions:

1.  Activists have asserted that there is a strong gender bias in student evaluations of professors, with male professors enjoying a boost in rating from this bias. While this has been celebrated by ideologues, skeptics have pointed out that this research is of technically poor quality, either due to a low sample size –as small as n = 1 (Mitchell & Martin, 2018), failure to control for confounders such as teaching experience (Centra & Gaubatz, 2000) or obvious p-hacking (MacNell et al., 2015). We would like you to answer the question whether there is evidence of a pro-male gender bias in this dataset. Hint: A significance test is probably required.
2. Is there a gender difference in the spread (variance/dispersion) of the ratings distribution? Again, it is advisable to consider the statistical significance of any observed gender differences in this spread.
3. What is the likely size of both of these effects (gender bias in average rating, gender bias in spread of average rating), as estimated from this dataset? Please use 95% confidence and make sure to report each/both.

## Data Loading
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.genmod.generalized_linear_model import GLM
from statsmodels.genmod.families import Binomial
from statsmodels.genmod.families.links import logit
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind, mannwhitneyu, chisquare
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Ridge, Lasso, LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, roc_auc_score, roc_curve
from hyperopt import hp, tpe, fmin, Trials, STATUS_OK
from scipy.stats import ks_2samp
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')


num_df_cols = ["Average Ratings", "Average Difficulty", "Number of Ratings", "Pepper", "Would Retake Prop", "Ratings from Online", "Male", "Female"]
qual_df_cols = ["Major", "University", "State"]
tags_df_cols = ["Tough Grader", "Good Feedback", "Respected", "Lots to Read", "Participation Matters", "Dont Skip Class", "Lots of Homework", "Inspirational", "Pop Quizzes", "Accessible", "So Many Papers", "Clear Grading", "Hilarious", "Test Heavy", "Graded by Few Things", "Amazing Lectures", "Caring", "Extra Credit", "Group Projects", "Lecture Heavy"]

num_df = pd.read_csv("rmpCapstoneNum.csv", header = None)
num_df.columns = num_df_cols
qual_df = pd.read_csv("rmpCapstoneQual.csv", header = None)
qual_df.columns = qual_df_cols
tags_df = pd.read_csv("rmpCapstoneTags.csv", header = None)
tags_df.columns = tags_df_cols

"""## Question 1

Activists have asserted that there is a strong gender bias in student evaluations of professors, with male professors enjoying a boost in rating from this bias. While this has been celebrated by ideologues, skeptics have pointed out that this research is of technically poor quality, either due to a low sample size –as small as n = 1 (Mitchell & Martin, 2018), failure to control for confounders such as teaching experience (Centra & Gaubatz, 2000) or obvious p-hacking (MacNell et al., 2015). We would like you to answer the question whether there is evidence of a pro-male gender bias in this dataset. Hint: A significance test is probably required.

"""

## 1. perform Mann-Whitney U test for each group
columns_to_extract = ["Average Ratings", "Number of Ratings", "Male", "Female"]
q1 = num_df[columns_to_extract].dropna()
q1 = q1[q1['Male'] != q1['Female']]
q1 = q1.drop(columns=['Male'])
female_q1 = q1[q1['Female'] == 1]
male_q1 = q1[q1['Female'] == 0]
bins = [5, 10, 15, 20, 25]
labels = ["5-9", "10-14", "15-19", "20-24"]
q1['Rating Range'] = pd.cut(q1['Number of Ratings'], bins=bins, labels=labels, right=False)
female_q1 = q1[q1['Female'] == 1].dropna(subset=['Rating Range'])
male_q1 = q1[q1['Female'] == 0].dropna(subset=['Rating Range'])

# Unique values of Number of Ratings in the range 5 to 10
unique_ranges = sorted(q1['Rating Range'].dropna().unique())

# Perform Mann-Whitney U test for each unique Number of Ratings
u_test_results = []


for rating_range in unique_ranges:
    female_difficulties = female_q1[female_q1['Rating Range'] == rating_range]['Average Ratings']
    male_difficulties = male_q1[male_q1['Rating Range'] == rating_range]['Average Ratings']

    if len(female_difficulties) > 0 and len(male_difficulties) > 0:
        u_stat, p_value = mannwhitneyu(female_difficulties, male_difficulties, alternative='two-sided')
        u_test_results.append((rating_range, u_stat, p_value))
u_test_results_df = pd.DataFrame(u_test_results, columns=['Rating Range', 'U Statistic', 'P Value'])
print(u_test_results_df)

significant_results = u_test_results_df[u_test_results_df['P Value'] < 0.005]
print(significant_results)

## Q1 plot
ordered_ranges = ["5-9", "10-14", "15-19", "20-24"]

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for idx, rating_range in enumerate(ordered_ranges):
    ax = axes[idx]

    female_data = female_q1[female_q1['Rating Range'] == rating_range]['Average Ratings']
    male_data = male_q1[male_q1['Rating Range'] == rating_range]['Average Ratings']

    ax.hist(female_data, bins=10, alpha=0.5, label=f"Female", color='Orange', density=True)
    ax.hist(male_data, bins=10, alpha=0.5, label=f"Male", color='Purple', density=True)

    if not female_data.empty:
        female_median = female_data.median()
        ax.axvline(female_median, color='orange', linestyle='--', label=f"Median Female: {female_median:.2f}")
    if not male_data.empty:
        male_median = male_data.median()
        ax.axvline(male_median, color='purple', linestyle='--', label=f"Median Male: {male_median:.2f}")

    ax.set_title(f"{rating_range} Rating Range")
    ax.set_xlabel("Average Ratings")
    ax.set_ylabel("Density")
    ax.legend(loc='upper right')
    ax.grid()

plt.tight_layout()
plt.show()

"""## Question 2

Is there a gender difference in the spread (variance/dispersion) of the ratings distribution? Again, it is advisable to consider the statistical significance of any observed gender differences in this spread.

"""

## 2. KS test
columns_to_extract = ["Average Ratings", "Number of Ratings", "Male", "Female"]
q2 = num_df[columns_to_extract].dropna()
q2 = q2[q2['Male'] != q2['Female']]
q2 = q2.drop(columns=['Male'])
female_q2 = q2[q2['Female'] == 1]
male_q2 = q2[q2['Female'] == 0]
bins = [5, 10, 15, 20, 25]
labels = ["5-9", "10-14", "15-19", "20-24"]
q2['Rating Range'] = pd.cut(q2['Number of Ratings'], bins=bins, labels=labels, right=False)
female_q2 = q2[q1['Female'] == 1].dropna(subset=['Rating Range'])
male_q2 = q2[q1['Female'] == 0].dropna(subset=['Rating Range'])

# Unique rating ranges
unique_ranges = sorted(q2['Rating Range'].dropna().unique())

# Perform KS test for each unique rating range
ks_test_results = []

for rating_range in unique_ranges:
    female_ratings = female_q2[female_q2['Rating Range'] == rating_range]['Average Ratings']
    male_ratings = male_q2[male_q2['Rating Range'] == rating_range]['Average Ratings']

    if len(female_ratings) > 0 and len(male_ratings) > 0:
        ks_stat, p_value = ks_2samp(female_ratings, male_ratings)
        ks_test_results.append((rating_range, ks_stat, p_value))

# Convert results to DataFrame
ks_test_results_df = pd.DataFrame(ks_test_results, columns=['Rating Range', 'KS Statistic', 'P Value'])

print(ks_test_results_df)

significant_ks_results = ks_test_results_df[ks_test_results_df['P Value'] < 0.005]
print("\nSignificant Results:")
print(significant_ks_results)

## Q2. plot
ordered_ranges = ["5-9", "10-14", "15-19", "20-24"]

fig, axes = plt.subplots(len(ordered_ranges), 1, figsize=(12, 10), sharex=True, sharey=True)
plt.subplots_adjust(hspace=0.5)

for idx, rating_range in enumerate(ordered_ranges):
    ax = axes[idx]

    female_data = female_q1[female_q1['Rating Range'] == rating_range]['Average Ratings']
    male_data = male_q1[male_q1['Rating Range'] == rating_range]['Average Ratings']

    sns.kdeplot(female_data, ax=ax, fill=True, color="orange", alpha=0.5, label="Female", bw_adjust=0.8)
    sns.kdeplot(male_data, ax=ax, fill=True, color="purple", alpha=0.5, label="Male", bw_adjust=0.8)

    ax.set_title(f"Rating Range: {rating_range}")
    ax.set_ylabel("Density")
    ax.legend(loc="upper right")

plt.xlabel("Average Ratings")
plt.suptitle("Distribution of Average Ratings by Gender for Each Rating Range", y=0.93)
plt.show()

"""## Question 3

What is the likely size of both of these effects (gender bias in average rating, gender bias in spread of average rating), as estimated from this dataset? Please use 95% confidence and make sure to report each/both.

"""

## 3. Cohen's d and ratio of std
## and bootstrap
bins = [5, 10, 15, 20, 25]
labels = ["5-9", "10-14", "15-19", "20-24"]

q1['Rating Range'] = pd.cut(q1['Number of Ratings'], bins=bins, labels=labels, right=False)

female_q1 = q1[q1['Female'] == 1].dropna(subset=['Rating Range'])
male_q1 = q1[q1['Female'] == 0].dropna(subset=['Rating Range'])

unique_ranges = sorted(q1['Rating Range'].dropna().unique())

bootstrap_results = []
def calculate_cohens_d(sample_female, sample_male):
    mean_female = sample_female.mean()
    mean_male = sample_male.mean()
    std_female = sample_female.std()
    std_male = sample_male.std()
    pooled_std = np.sqrt(((len(sample_female) - 1) * std_female**2 + (len(sample_male) - 1) * std_male**2) /
                         (len(sample_female) + len(sample_male) - 2))
    return (mean_female - mean_male) / pooled_std if pooled_std > 0 else np.nan

def calculate_std_ratio(sample_female, sample_male):
    std_female = sample_female.std()
    std_male = sample_male.std()
    return std_female / std_male if std_male > 0 else np.nan



for rating_range in unique_ranges:
    female_ratings = female_q1[female_q1['Rating Range'] == rating_range]['Average Ratings']
    male_ratings = male_q1[male_q1['Rating Range'] == rating_range]['Average Ratings']

    if len(female_ratings) > 1 and len(male_ratings) > 1:
        # Calculate Cohen's d
        cohens_d = calculate_cohens_d(female_ratings, male_ratings)

        # Calculate ratio of standard deviations
        std_ratio = calculate_std_ratio(female_ratings, male_ratings)

        # Bootstrapping for 95% confidence intervals
        bootstrap_d_values = []
        bootstrap_std_ratio_values = []
        for _ in range(10000):
            boot_female = female_ratings.sample(frac=1, replace=True)
            boot_male = male_ratings.sample(frac=1, replace=True)
            bootstrap_d_values.append(calculate_cohens_d(boot_female, boot_male))
            bootstrap_std_ratio_values.append(calculate_std_ratio(boot_female, boot_male))

        d_ci_lower, d_ci_upper = np.percentile(bootstrap_d_values, [2.5, 97.5])
        std_ci_lower, std_ci_upper = np.percentile(bootstrap_std_ratio_values, [2.5, 97.5])

        bootstrap_results.append((rating_range, cohens_d, d_ci_lower, d_ci_upper, std_ratio, std_ci_lower, std_ci_upper))

# Convert results to DataFrame
bootstrap_results_df = pd.DataFrame(
    bootstrap_results,
    columns=['Rating Range', 'Cohen\'s d', 'Cohen\'s d CI Lower', 'Cohen\'s d CI Upper',
             'Std Ratio', 'Std Ratio CI Lower', 'Std Ratio CI Upper']
)

ordered_ranges = ["5-9", "10-14", "15-19", "20-24"]
bootstrap_results_df['Rating Range'] = pd.Categorical(bootstrap_results_df['Rating Range'], categories=ordered_ranges, ordered=True)
bootstrap_results_df = bootstrap_results_df.sort_values('Rating Range')

print("Bootstrap Results:")
print(bootstrap_results_df.to_string(index=False))

## Q3. plot
fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=False)

# Plot Cohen's d with confidence intervals
axes[0].bar(
    bootstrap_results_df['Rating Range'],
    bootstrap_results_df['Cohen\'s d'],
    yerr=[
        bootstrap_results_df['Cohen\'s d'] - bootstrap_results_df['Cohen\'s d CI Lower'],
        bootstrap_results_df['Cohen\'s d CI Upper'] - bootstrap_results_df['Cohen\'s d']
    ],
    capsize=5,
    color='cadetblue',
    edgecolor='black'
)
axes[0].set_title("Cohen's d with 95% Confidence Intervals")
axes[0].set_xlabel("Rating Range")
axes[0].set_ylabel("Cohen's d")
axes[0].grid(axis='y', linestyle='--', alpha=0.7)

# Plot Standard Deviation Ratio with confidence intervals
axes[1].bar(
    bootstrap_results_df['Rating Range'],
    bootstrap_results_df['Std Ratio'],
    yerr=[
        bootstrap_results_df['Std Ratio'] - bootstrap_results_df['Std Ratio CI Lower'],
        bootstrap_results_df['Std Ratio CI Upper'] - bootstrap_results_df['Std Ratio']
    ],
    capsize=5,
    color='darkkhaki',
    edgecolor='black'
)
axes[1].set_title("Standard Deviation Ratio with 95% Confidence Intervals")
axes[1].set_xlabel("Rating Range")
axes[1].set_ylabel("Standard Deviation Ratio")
axes[1].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

